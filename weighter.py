import torch
import pandas as pd
from transformers import BertTokenizer, BertModel
import matplotlib.pyplot as plt
from sentence_transformers import SentenceTransformer, util, models, losses
from sklearn.model_selection import train_test_split


model = SentenceTransformer('paraphrase-MiniLM-L12-v2')
bert = models.Transformer('bert-base-uncased')

pooler = models.Pooling(
    bert.get_word_embedding_dimension(),
    pooling_mode_mean_tokens=True
)

custom_model = SentenceTransformer(modules=[bert, pooler])

# Two lists of sentences
sentences1 = ['The cat sits outside',
             'A man is playing guitar',
             'The new movie is awesome']

sentences2 = ['The dog plays in the garden',
              'A woman watches TV',
              'The new movie is so great']

absract1 = ['Ripples are brief high-frequency electrographic events with important roles in episodic memory. However, the in vivo circuit mechanisms coordinating ripple-related activity among local and distant neuronal ensembles are not well understood. Here, we define key characteristics of a long-distance projecting GABAergic cell group in the mouse hippocampus that selectively exhibits high-frequency firing during ripples while staying largely silent during theta-associated states when most other GABAergic cells are active. The high ripple-associated firing commenced before ripple onset and reached its maximum before ripple peak, with the signature theta-OFF, ripple-ON firing pattern being preserved across awake and sleep states. Controlled by septal GABAergic, cholinergic, and CA3 glutamatergic inputs, these ripple-selective cells innervate parvalbumin and cholecystokinin-expressing local interneurons while also targeting a variety of extra-hippocampal regions. These results demonstrate the existence of a hippocampal GABAergic circuit element that is uniquely positioned to coordinate ripple-related neuronal dynamics across neuronal assemblies.']
abstractlike1 = ['Sharp wave ripples (SWRs) are high-frequency synchronization events generated by hippocampal neuronal circuits during various forms of learning and reactivated during memory consolidation and recall. There is mounting evidence that SWRs are essential for storing spatial and social memories in rodents and short-term episodic memories in humans. Sharp wave ripples originate mainly from the hippocampal CA3 and subiculum, and can be transmitted to modulate neuronal activity in cortical and subcortical regions for long-term memory consolidation and behavioral guidance. Different hippocampal subregions have distinct functions in learning and memory. For instance, the dorsal CA1 is critical for spatial navigation, episodic memory, and learning, while the ventral CA1 and dorsal CA2 may work cooperatively to store and consolidate social memories. Here, we summarize recent studies demonstrating that SWRs are essential for the consolidation of spatial, episodic, and social memories in various hippocampal-cortical pathways, and review evidence that SWR dysregulation contributes to cognitive impairments in neurodegenerative and neurodevelopmental diseases']
abstractnotlike1 = ['Memories are believed to be encoded by sparse ensembles of neurons in the brain. However, it remains unclear whether there is functional heterogeneity within individual memory engrams, i.e., if separate neuronal subpopulations encode distinct aspects of the memory and drive memory expression differently. Here, we show that contextual fear memory engrams in the mouse dentate gyrus contain functionally distinct neuronal ensembles, genetically defined by the Fos- or Npas4-dependent transcriptional pathways. The Fos-dependent ensemble promotes memory generalization and receives enhanced excitatory synaptic inputs from the medial entorhinal cortex, which we find itself also mediates generalization. The Npas4-dependent ensemble promotes memory discrimination and receives enhanced inhibitory drive from local cholecystokinin-expressing interneurons, the activity of which is required for discrimination. Our study provides causal evidence for functional heterogeneity within the memory engram and reveals synaptic and circuit mechanisms used by each ensemble to regulate the memory discrimination-generalization balance.']
abstractnotatalllike1 = ['This article describes neural models of attention. Since attention is not a disembodied process, the article explains how brain processes of consciousness, learning, expectation, attention, resonance, and synchrony interact. These processes show how attention plays a critical role in dynamically stabilizing perceptual and cognitive learning throughout our lives. Classical concepts of object and spatial attention are replaced by mechanistically precise processes of prototype, boundary, and surface attention. Adaptive resonances trigger learning of bottom-up recognition categories and top-down expectations that help to classify our experiences, and focus prototype attention upon the patterns of critical features that predict behavioral success. These feature-category resonances also maintain the stability of these learned memories. Different types of resonances induce functionally distinct conscious experiences during seeing, hearing, feeling, and knowing that are described and explained, along with their different attentional and anatomical correlates within different parts of the cerebral cortex. All parts of the cerebral cortex are organized into layered circuits. Laminar computing models show how attention is embodied within a canonical laminar neocortical circuit design that integrates bottom-up filtering, horizontal grouping, and top-down attentive matching. Spatial and motor processes obey matching and learning laws that are computationally complementary to those obeyed by perceptual and cognitive processes. Their laws adapt to bodily changes throughout life, and do not support attention or conscious states.']


# Load pre-trained model tokenizer (vocabulary)
#tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

def weighter_k(first_keywords, second_keywords):
    # keywords_1 = df.loc[ df['pubmed_id'] == first_id, 'keywords' ].iloc[0]
    # keywords_2 = df.loc[ df['pubmed_id'] == second_id, 'keywords' ].iloc[0]

    embeddings1 = model.encode(first_keywords, convert_to_numpy=True, normalize_embeddings = True)
    embeddings2 = model.encode(second_keywords, convert_to_numpy=True, normalize_embeddings = True)
    #print(embeddings1, embeddings1.shape)
    #print(embeddings2, embeddings2.shape)

    #Compute cosine-similarits
    cosine_scores = util.pytorch_cos_sim(embeddings1, embeddings2)
    return cosine_scores #1/(cosine_scores + 1)

    # dot_scores = util.dot_score(embeddings1, embeddings2)
    # return (dot_scores)



print(weighter_k(abstractlike1, absract1))
print(weighter_k(abstractnotlike1, absract1))
print(weighter_k(abstractlike1, abstractnotlike1))
print(weighter_k(abstractlike1, abstractnotatalllike1))

    #Output the pairs with their score
    # for i in range(len(sentences1)):
    #     print("{} \t\t {} \t\t Score: {:.4f}".format(sentences1[i], sentences2[i], cosine_scores[i][i]))
    #     print(keywords_1)



'''
import datasets

snli = datasets.load_dataset('snli', split='train')
m_nli = datasets.load_dataset('glue', 'mnli', split='train')

m_nli = m_nli.remove_columns(['idx'])
snli = snli.cast(m_nli.features)

nli = datasets.concatenate_datasets([snli, m_nli])
del snli, m_nli
# and remove bad rows
nli = nli.filter(
    lambda x: False if x['label'] == -1 else True
)

from sentence_transformers import InputExample
from tqdm.auto import tqdm  # so we see progress bar

train_samples = []
for row in tqdm(nli):
    train_samples.append(InputExample(
        texts=[row['premise'], row['hypothesis']],
        label=row['label']
    ))

from torch.utils.data import DataLoader

batch_size = 16

loader = DataLoader(
    train_samples, shuffle=True, batch_size=batch_size)


epochs = 1
warmup_steps = int(len(loader) * epochs * 0.1)

loss = losses.TripletLoss(model=custom_model) #SoftmaxLoss

custom_model.fit(
    train_objectives=[(loader, loss)],
    epochs=epochs,
    warmup_steps=warmup_steps,
    output_path='./sbert_test_b',
    show_progress_bar=False,
)'''

# from datasets import load_dataset

# dataset = load_dataset("owaiskha9654/PubMed_MultiLabel_Text_Classification_Dataset_MeSH")

dataset_Name='/PubMed Multi Label Text Classification Dataset Processed.csv'

df= pd.read_csv(dataset_Name)


df_train, df_test = train_test_split(df, random_state=32, test_size=0.20, shuffle=True)

cols = df.columns
cols = list(df.columns)
mesh_Heading_categories = cols[6:]
num_labels = len(mesh_Heading_categories)

df_train['one_hot_labels'] = list(df_train[mesh_Heading_categories].values)

labels = list(df_train.one_hot_labels.values)
Article_train = list(df_train.abstractText.values)











######################################################################################
'''
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

all_cols = ['label']

for part in ['premise', 'hypothesis']:
    dataset = dataset.map(
        lambda x: tokenizer(
            x[part], max_length=128, padding='max_length',
            truncation=True
        ),
          batched=True
    )
    for col in ['input_ids', 'attention_mask']:
        dataset = dataset.rename_column(
            col, part+'_'+col
        )
        all_cols.append(part+'_'+col)
print(all_cols)


dataset.set_format(type='torch', columns=all_cols)

# initialize the dataloader
batch_size = 16
loader = torch.utils.data.DataLoader(
    dataset, batch_size=batch_size, shuffle=True
)


def mean_pool(token_embeds, attention_mask):
    # reshape attention_mask to cover 768-dimension embeddings
    in_mask = attention_mask.unsqueeze(-1).expand(
        token_embeds.size()
    ).float()
    # perform mean-pooling but exclude padding tokens (specified by in_mask)
    pool = torch.sum(token_embeds * in_mask, 1) / torch.clamp(
        in_mask.sum(1), min=1e-9
    )
    return pool'''